batch_size,dropout,epochs,layer 1 activation,layer 1 nodes,layer 2 activation,layer 2 nodes,lr,maxnorm,output activation,patience,accuracy,std
32,0.2,500,relu,8,relu,8,0.001,3,sigmoid,32,0.7291666666666666,0.01473139127471974
32,0.2,500,relu,8,relu,8,0.01,3,sigmoid,32,0.8125,0.07654655446197431
32,0.2,500,relu,8,relu,16,0.001,3,sigmoid,32,0.7395833333333334,0.05311478659992484
32,0.2,500,relu,8,relu,16,0.01,3,sigmoid,32,0.78125,0.05103103630798288
32,0.2,500,relu,16,relu,8,0.001,3,sigmoid,32,0.71875,0.02551551815399144
32,0.2,500,relu,16,relu,8,0.01,3,sigmoid,32,0.7083333333333334,0.02946278254943948
32,0.2,500,relu,16,relu,16,0.001,3,sigmoid,32,0.8125,0.02551551815399144
32,0.2,500,relu,16,relu,16,0.01,3,sigmoid,32,0.7604166666666666,0.03897559777889522
